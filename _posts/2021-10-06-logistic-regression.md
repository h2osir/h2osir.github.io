---
layout: post
author: H2osir
title: 逻辑回归的基本原理
date: 2021-10-06 21:45:35 
categories: [机器学习算法原理, 逻辑回归]
tags: [逻辑回归, 机器学习]
math: true
mermaid: true
---

----

逻辑回归是统计学习中的经典分类方法。属于对数线型模型。分为二项逻辑回归和多项逻辑回归，这里主要介绍二项逻辑回归模型。

### 二项逻辑回归模型

二项逻辑回归模型，即二分类模型。由条件概率分布 $P(Y\|X)$ 表示，随机变量 $X$ 的取值为实数，随机变量 $Y$ 的取值为 1 或 0。

二项逻辑回归模型是如下的条件概率分布：

$$P(Y=1|x)=\frac{e^{w·x+b}}{1+e^{w·x+b}}\quad\quad\quad\quad\quad\quad\quad(1)$$

$$P(Y=0|x)=\frac{1}{1+e^{w·x+b}}\quad\quad\quad\quad\quad\quad\quad(2)$$

这里，$x \in R^n$ 是输入，$Y \in \{0,1\}$ 是输出，$w \in R^n$ 和 $b \in R$ 是参数，$w$ 称为权值向量，$b$ 称为偏置，$w·x$ 为 $w$ 和 $x$ 的内积。

逻辑回归比较两个概率值 $P(Y=1\|x)$ 与 $P(Y=0\|x)$ 的大小，将实例 $x$ 分到概率值较大的那一类。

有时为了方便，将权值向量和输入向量加以扩充，仍记作 $w,x$ ，即 $w=(w^{(1)},w^{(2)},...,w^{(n)},b)^T,x=(x^{(1)},x^{(2)},...,x^{(n)},1)^T$ 。此时：

$$P(Y=1|x)=\frac{e^{w·x}}{1+e^{w·x}}$$

$$P(Y=0|x)=\frac{1}{1+e^{w·x}}$$

现在考察逻辑回归模型的特点。一个事件的几率（odds）是指该事件发生的概率与该事件不发生的概率的比值。如果事件发生的概率为 $p$ ，那么该事件的几率是 $\frac{p}{1-p}$ ，该事件的对数几率（log odds）或 logit 函数（很多资料叫 sigmoid 函数）是

$$logit(p)=\log\frac{p}{1-p}$$

对逻辑回归而言，

$$\log \frac{P(Y=1|x)}{1-P(Y=1|x)}=w·x$$

这就是说，在逻辑回归模型中，输出 $Y=1$ 的对数几率是输入 $x$ 的线性函数。或者说，输出 $Y=1$ 的对数几率是由输入 $x$ 的线性函数表示的模型，即逻辑回归模型。

### 模型参数估计

#### 极大化似然函数估计

极大化似然函数估计，又叫 ”极大似然估计“ 。是根据求解关于 $w，这里 w=(w^{(1)},w^{(2)},...,w^{(n)}, b)^T$  的函数的极大值来得到 $w$ 的估计值 $\widehat w$ 。

逻辑回归模型学习时，应用极大似然估计法估计模型参数，从而得到逻辑回归模型。

设：

$$P(Y=1|x)=\pi (x), \quad P(Y=0|x)=1- \pi (x)$$

由于 $Y$ 只能取 0 或者 1 ，则似然函数可以巧妙地写成下面这种形式

$$\prod_{i=1}^N [\pi (x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}$$

其中，$N$ 是样本容量，$y_i$ 是第 $i$ 个样本的真实类别（0 或 1），$\pi (x_i)$ 是模型对第 $i$ 个样本的预测为类别 1 的概率（下同）。

对上式取对数，则对数似然函数为

$$\begin{equation} \begin{aligned} L(w)&=\sum_{i=1}^N [y_i \log \pi(x_i) + (1-y_i)\log(1-\pi(x_i))] \\\\
&=\sum_{i=1}^N [y_i \log \frac{\pi(x_i)}{1-\pi(x_i)} + \log(1-\pi(x_i))] \\\\
&=\sum_{i=1}^N [y_i(w·x_i)-\log(1+e^{w·x_i})]\end{aligned} \end{equation}$$

上式中，$x_i, y_i$ 均已知，$w$ 为未知变量，对 $L(w)$ 求极大值，得到 $w$ 的估计值 $\widehat w$ ，代入上面的（1），（2）式即为学习到的逻辑回归模型。

#### 极小化损失函数估计

在常用的损失函数中，逻辑回归采用的损失函数为**对数损失函数**或**对数似然损失函数**，其公式为：

$$L(Y, P(Y|X))= -\log{P(Y|X)}$$

而对于二分类逻辑回归模型，得到的 $y$ 要么是1，要么是0。假设对于样本 $x$ ，估计 $y=1$ 的概率是 $\widehat p$ ，则可以将损失函数分成两类：

- 如果 $x$ 的真实类别 $y=1$，则：估计出来的概率 $\widehat p$ 越小，损失函数就越大（估计错误）
- 如果 $x$ 的真实类别 $y=0$，则：估计出来的概率 $\widehat p$ 越大，损失函数就越大（估计错误）

那么根据对数损失函数的定义公式，损失函数可写成如下形式：

$$J=\begin{cases}-\log \widehat p\quad\quad\quad\quad\quad if\quad Y=1\\\\ -\log(1- \widehat p)\quad\quad\ \,\ if \quad Y=0\end{cases}$$

由于模型是个二分类问题，分类结果 $y$ 非 0 即 1 ，因此我们可以使用一个巧妙的方法，通过控制系数的方式，将上面的两个式子合并成一个：

$$J(\widehat p,y)=-\log(\widehat p)^y-\log(1-\widehat p)^{1-y}$$

化简上式，得

$$J(\widehat p,y)=-y\log(\widehat p)-(1-y)\log(1-\widehat p)$$

以上是对于单个样本的损失函数，那么扩展到整个数据集，其损失函数则是：

$$\begin{equation} \begin{aligned}J(\widehat p,y)&=-\frac{1}{N}\sum_{i=1}^N [y_{i}\log(\widehat p^{(i)})+(1-y_{i})\log(1-\widehat p^{(i)})]\\\\
&=-\frac{1}{N}\sum_{i=1}^N [y_i \log \frac{\widehat p^{(i)}}{1-\widehat p^{(i)}} + \log(1-\widehat p^{(i)})]\end{aligned} \end{equation}$$

这里，$N$ 是样本容量，$y_i$ 是第 $i$ 个样本的真实类别，$\widehat p^{(i)}$ 是模型对第 $i$ 个样本的预测为类别 1 的概率。

将下面两式

$$\log \frac{\widehat p^{(i)}}{1-\widehat p^{(i)}} = w·x_i,\quad (1-\widehat p^{(i)})=\frac{1}{1+e^{w·x_i}}$$

代入 $J(\widehat p, y)$ ，得：

$$J(w)=-\frac{1}{N} \sum_{i=1}^N [y_i(w·x_i)-\log(1+e^{w·x_i})]$$

这就是**逻辑回归的损失函数**。通过求解 $J(w)$ 的最小值，来估计参数 $w$ 的估计值。

### 最优化求解算法

现在，我们知道了逻辑回归模型中估计参数的两种方法，极大化似然函数估计和极小化损失函数估计。那么如何求解这个最值？

这是一个多元函数的无约束最值求解问题，$J(w)$ 中的 $w=(w^{(1)},w^{(2)},...,w^{(n)}, b)^T$ ，含有 $n+1$ 个元素，我们需要通过求解函数的最值来估计出 $(w^{(1)},w^{(2)},...,w^{(n)}, b)^T$ 中各个元素的估计值，作为最终的模型参数。

这里主要介绍一种常用的方法——**梯度下降法**。梯度下降法是一种求解函数无约束最小值问题的方法；而**梯度上升法**则是求解函数无约束最大值问题的方法。二者都属于迭代算法，其迭代公式分别为：

梯度下降法：

$$w:=w-\alpha \frac{\partial J(w)}{\partial w}$$

梯度上升法：

$$w:=w+\alpha \frac{\partial J(w)}{\partial w}$$

这里引入两个概念：

- 梯度（或梯度方向）

  上式中的 $\frac{\partial J(w)}{\partial w}$ 就是梯度，它的结果也是一个向量，含有 $n+1$ 个元素，它表示下一步迭代的方向。即下降（或上升）的方向。有了方向，我们还需要知道在这个方向上”走多远"，这就是下面提到的步长。

- 步长（或学习率）

  上式中的 $\alpha$ 称为步长（学习率），该值太大局部线性近似就不成立，偏差较大；该值太小收敛太慢，导致一个长时间的训练。所以后续调参需要注意该值的选择。

对于给到的一个初始值 $w_0$ ，$J(w)$ 对 $w_0$ 中的每一个元素求一阶偏导，便会得到 $J(w)$ 在 $w_0$ 中每一个元素处的一阶偏导值，再由迭代公式进行迭代，直到 $J(w)$ 收敛。

## 总结

关于最优化求解算法，除了梯度下降和梯度上升之外，还有其他的算法，比如牛顿法、拟牛顿法等。深度地理解某一种算法，对于理解其他算法也会有一定的帮助。

