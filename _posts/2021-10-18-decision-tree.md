---
title: 决策树的基本原理
author: H2osir
date: 2021-10-18 21:19:00 +0800
categories: [机器学习算法原理, 决策树]
tags: [机器学习, 决策树]
math: true
mermaid: true
---

决策树是一种基本的分类与回归方法。此处简单介绍分类的决策树。决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是 if-then 规则的集合。其主要**优点**是模型具有可读性，分类速度快。学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。预测时，对新的数据，利用决策树模型进行分类。决策树学习通常包括 **3 个步骤：**特征选择、决策树的生成和决策树的修剪。

### 1. 特征选择

特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率。通常特征选择的准则是**信息增益** 或 **信息增益比** 。

举个例子来说明特征选择问题

<center>表1. 贷款信息表</center>

| ID   | 年龄 | 有工作 | 有自己的房子 | 信贷情况 | 类别 |
| ---- | ---- | ------ | ------------ | -------- | ---- |
| 1    | 青年 | 否     | 否           | 一般     | 否   |
| 2    | 青年 | 否     | 否           | 好       | 否   |
| 3    | 青年 | 是     | 否           | 好       | 是   |
| 4    | 青年 | 是     | 是           | 一般     | 是   |
| 5    | 青年 | 否     | 否           | 一般     | 否   |
| 6    | 中年 | 否     | 否           | 一般     | 否   |
| 7    | 中年 | 否     | 否           | 好       | 否   |
| 8    | 中年 | 是     | 是           | 好       | 是   |
| 9    | 中年 | 否     | 是           | 非常好   | 是   |
| 10   | 中年 | 否     | 是           | 非常好   | 是   |
| 11   | 老年 | 否     | 是           | 非常好   | 是   |
| 12   | 老年 | 否     | 是           | 好       | 是   |
| 13   | 老年 | 是     | 否           | 好       | 是   |
| 14   | 老年 | 是     | 否           | 非常好   | 是   |
| 15   | 老年 | 否     | 否           | 一般     | 否   |

表1是一个由15个样本组成的贷款申请训练数据。希望通过所给的训练数据学习一个贷款申请的决策树，用以对未来的贷款申请进行分类，即当新的客户提出贷款申请时，根据申请人的特征利用决策树决定是否批准贷款申请。

特征选择是决定用哪个特征来划分特征空间。

#### 信息增益

为了便于说明，先给出**熵**与**条件熵**的定义。

在信息论与概率统计中，熵（entropy）是表示随机变量不确定性的度量。设 $X$ 是一个取有限个值的离散随机变量，其概率分布为

$$P(X=x_i)=p_i, i=1,2,...,n$$

则随机变量 $X$ 的熵定义为

$$H(X)=-\sum_{i=1}^{n}p_i·\log_2p_i$$

上式中对数的底数可以是2或者e，这时熵的单位分别称作比特（bit）或纳特（nat）。

若 $p_i=0$ ，则定义 $0log_20=0$ 。由定义可知，熵只依赖与 $X$ 的分布，而与 $X$ 的取值无关，所以也可以将 $X$ 的熵记作 $H(p)$ ，即：

$$H(p)=-\sum_{i=1}^{n}p_i·\log_2p_i$$

熵越大，随机变量的不确定性就越大。从定义可验证

$$0\leq{H(p)}\leq{\log_2n}$$

当随机变量只取两个值，例如 1，0 时，即 $X$ 的分布为

$$P(X=1)=p,\quad P(X=0)= 1-p,\quad 0 \leq p \leq 1$$

则 $X$ 的熵为

$$H(p)=-p\log_2p-(1-p)\log_2(1-p)$$

设有随机变量 $(X,Y)$ ，其联合概率分布为

$$P(X=x_i,Y=y_i)=p_{ij}, \quad i=1,2,...,n;\quad j=1,2,...,m$$

则随机变量 $X$ 给定的条件下随机变量 $Y$ 的条件熵 $H(Y\|X)$ 定义为：$X$ 给定条件下 $Y$ 的条件概率分布的熵对 $X$ 的数学期望

$$H(Y|X)=\sum_{i=1}^{n}p_i·H(Y|X=x_i)$$

这里，$p_i=P(X=x_i),i=1,2,...,n$ 。

**信息增益**（information gain）表示得知特征 $X$ 的信息而使得类 $Y$ 的信息的不确定性减少的程度。

特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D,A)$，定义为集合 $D$ 的熵 $H(D)$ 与特征 $A$ 给定的条件下 $D$ 的条件熵 $H(D\|A)$ 之差，即：

$$g(D,A)=H(D)-H(D|A)$$

显然，对于数据集 $D$ 而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益。信息增益越大的特征具有更强的分类能力。因此，根据信息增益准则的特征选择方法是：对训练数据集（或子集）$D$ ，计算其每个特征的信息增益，并比较他们的大小，选择信息增益最大的特征。

*前方高能，请打开脑洞！！！*

**设：**

- 训练数据集为 $D$，$\|D\|$表示其样本容量，即样本的个数。

- 有 $K$ 个类别 $C_k ，k=1,2,...,K$，$\|C_k\|$属于类 $C_k$ 的样本个数，则有：
  
  $$\sum_{k=1}^{K}|C_k|=|D|$$
  
- 特征 $A$  有 $n$ 个不同的取值 $\{a_1,a_2,...,a_n\}$ ，根据特征 $A$ 的取值，将 $D$ 划分为 $n$ 个子集 $D_1,D_2,...,D_n$，$\|D_i\|$ 为 $D_i$ 的样本个数，则有：
  
  $$\sum_{i=1}^n{|D_i|}=|D|$$
  
  记子集 $D_i$ 中属于类 $C_k$ 的样本的集合为 $D_{ik}$ ，即$D_{ik}=D_i\bigcap{C_k}$，$\|D_{ik}\|$为 $D_{ik}$ 的样本个数。

于是信息增益的算法如下。

**信息增益的算法**

1. 计算数据集 $D$  的熵（经验熵）$H(D)$
   
   $$H(D)=-\sum_{k=1}^K\frac{|C_k|}{|D|}\log_2 \frac{|C_k|}{|D|}$$
   
2. 计算特征 $A$ 对数据集 $D$ 的条件熵（经验条件熵）$$H(D\|A)$$
   
   $$H(D|A)=\sum_{i=1}^n \frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n \frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}\log_2 \frac{|D_{ik}|}{|D_i|}$$
   
3. 计算信息增益
   
   $$g(D,A)=H(D)-H(D|A)$$

**举个栗子：**对表1 所给的训练数据集 $D$ ，根据信息增益准则选择最优特征。

**解：**首先计算数据集 $D$ 的熵 $H(D)$ 。

$$H(D)=-\frac{9}{15} \log_2{\frac{9}{15}}-\frac{6}{15} \log_2{\frac{6}{15}}=0.971$$

然后计算各特征对数据集 $D$ 的信息增益。分别以 $A_1,A_2,A_3,A_4$ 表示年龄、有工作、有自己的房子和信贷情况 4 个特征，则：

（1）分别以 $1,2,3$ 表示 $A_1$ 的三个取值，则：

$$p_1 = \frac{5}{15},\quad p_2 = \frac{5}{15},\quad p_3 = \frac{5}{15}$$

然后计算特征 $A_1$ 对数据集 $D$ 的条件熵 $H(D\|A_1)$ ，即：

$$\begin{equation} \begin{aligned} H(D|A_1)&=p_1H(D_1)+p_2H(D_2)+p_3H(D_3)\\
&=[\frac{5}{15}(-\frac{2}{5}\log_2\frac{2}{5}-\frac{3}{5}\log_2\frac{3}{5}) + \frac{5}{15}(-\frac{3}{5}\log_2\frac{3}{5}-\frac{2}{5}\log_2\frac{2}{5}) + \frac{5}{15}(-\frac{4}{5}\log_2\frac{4}{5}-\frac{1}{5}\log_2\frac{1}{5})]\\ &=0.888 \end{aligned} \end{equation}$$

接下来计算特征 $A_1$ 对数据集 $D$ 的信息增益，即：

$$\begin{equation} \begin{aligned} g(D,A_1)&=H(D)=H(D|A_1)\\
&=0.971-0.888\\
&=0.083 \end{aligned} \end{equation}$$

（2）

$$\begin{equation} \begin{aligned} g(D,A_2)&=H(D)-[\frac{5}{15}H(D_1)+\frac{10}{15}H(D_2)]\\
&=0.971-[\frac{5}{15}\times0+\frac{10}{15}(-\frac{4}{10}\log_2\frac{4}{10}-\frac{6}{10}\log_2\frac{6}{10})]\\
&=0.324 \end{aligned} \end{equation}$$

同理，可以算出（3）和（4），分别是：

$$g(D,A_3)=0.420,\quad g(D,A_4)=0.363$$

最后，比较各特征的信息增益值。由于特征 $A_3$ 的信息增益值最大，所以选择特征 $A_3$ 作为最优特征。

#### 信息增益比

以信息增益作为划分训练集的特征的依据，存在偏向于选择取值较多的特征的问题，因为取值越多，信息增益就会越大。而使用信息增益比（information gain ratio）可以对这一问题进行校正。这是特征选择的另一准则。

特征 $A$ 对训练数据集 $D$ 的**信息增益比** $g_R(D,A)$，定义为其信息增益 $g(D,A)$ 与训练集 $D$ 关于特征 $A$ 的值的熵 $H_A(D)$ 之比，即

$$g_R(D,A)=\frac{g(D,A)}{H_A(D)}$$

其中，

$$H_A(D)=-\sum_{i=1}^n \frac{|D_i|}{|D|} \log_2 \frac{|D_i|}{|D|}$$

$n$ 是特征 $A$ 取值的个数。

### 2. 决策树的生成

#### 2.1 ID3 算法

**ID3** 算法的核心是在决策树各个结点上应用信息增益正则选择特征，递归地构建决策树。具体方法是：从根节点（root node）开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点；再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小（小于某个阈值）或没有特征可以选择为止。最后得到一颗决策树。**ID3** 相当于用极大似然法进行概率模型的选择。

**ID3** 算法只有树的生成，所以该算法生成的树容易产生过拟合。

#### 2.2 C4.5 算法

**C4.5** 算法与 **ID3** 算法相似，**C4.5** 算法对 **ID3** 算法进行了改进。**C4.5** 在生成的过程中，采用**信心增益比**准则来选择特征。

### 3. 决策树的剪枝

决策树生成算法递归地产生决策树，直到不能继续下去为止。这样产生的决策树往往对训练集数据的分类很准确，但对未知的测试数据的分类却没有那么准确，即出现过拟合现象。解决过拟合的办法是考虑决策树的复杂度，对已生成的决策树进行简化。

在决策树学习中将已生成的树进行简化的过程称为剪枝（pruning）。这里介绍一种简单的决策树的剪枝算法。

决策树的剪枝往往通过极小化决策树整体的损失函数来实现。设：树 $T$ 的叶结点个数为 $\|T\|$ ，$t$ 是树 $T$ 的叶结点，该叶结点有 $N_t$ 个样本点，其中 $k$ 类的样本点有 $N_{tk}$ 个，$k=1,2,...,K$，$$Ht(T)$$ 为叶结点 $t$ 上的熵，$\alpha \geq 0$ 为参数，则决策树学习的损失函数可以定义为

$$C_\alpha (T)=\sum_{t=1}^{|T|} N_tH_t(T)+\alpha|T|$$

其中，

$$H_t(T)=-\sum_{k=1}^K \frac{N_{tk}}{N_t} \log_2 \frac{N_{tk}}{N_t}$$

在损失函数中，将等式右端的第 1 项记作 $C(T)$ ，这时有：

$$C_\alpha (T)=C(T)+\alpha|T|$$

$C(T)$ 表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，$\|T\|$ 表示模型复杂度，参数 $\alpha \geq 0$ 控制两者之间的影响 。

可以看出，决策树生成只考虑了通过提高信息增益（或信息增益比）对训练数据进行更好的拟合。而决策树剪枝通过优化损失函数还考虑了减小模型复杂度。决策树生成学习局部的模型，而决策树剪枝学习整体的模型。

损失函数的极小化等价与正则化的极大似然估计。所以，利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。

**决策树的剪枝算法：**

- （1）计算每个结点的熵

- （2）递归地从树的叶结点向上回缩

  设一组叶结点回缩到其父结点之前与之后的整体树分别是 $T_B$ 与 $T_A$ ，其对应的损失函数值分别是 $C_\alpha(T_B)$ 与 $C_\alpha(T_A)$ ，如果 $C_\alpha(T_B) \geq C_\alpha(T_A)$ ，则进行剪枝，即将父结点变为新的叶结点。

- （3）返回（2），直到不能继续为止，得到损失函数最小的子树 $T_\alpha$ 。

### 4. CART 算法

分类与回归（classification and regression tree， CART）模型由 Breiman 等人在1984年提出，是应用广泛的决策树学习方法。

**CART** 算法假设决策树是二叉树，内部结点的特征的取值为 “是” 和 “否”，左分支是取值为 “是” 的分支，右分支是取值为 “否” 的分支。

前面介绍的 **ID3** 和 **C4.5** 算法，都只是生成决策树。而 **CART** 算法由两步组成：

（1）决策树生成：基于训练集生成决策树，生成的决策树要尽量大；

（2）决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。

#### 4.1 CART 生成

决策树的生成就是递归地构建二叉决策树的过程。对回归树用平方误差最小化准则，对分类树用基尼指数（Gini index）最小化准则，进行特征选择，生成二叉树。此处只介绍分类树的生成

##### 4.1.1 回归树的生成

假设 $X$ 与 $Y$ 分别为输入和输出变量，并且 $Y$ 是连续变量。考虑如何生成回归树。

选择第 $j$ 个变量 $x^j$ 和它的一个取值 $s$ ，作为切分变量和切分点，并定义两个区域：

$$R_1(j,s) = \{x|x^{(j)}\leq s\},\quad R_2(j,s) = \{x|x^{(j)} > s\}$$

然后寻找最优变量 $j$ 和最优的切分点 $s$ 。

具体地，求解

$$\min_{j,s} [\min_{c_1} \sum_{x_i \in R_{1(j,s)}}(y_i-c_1)^2+\min_{c_2} \sum_{x_i \in R_{2(j,s)}}(y_i-c_2)^2]$$

用平方误差最小的准则求解每个单元上的最优输出值。易知，单元 $R_m$ 上的 $c_m$ 的最优值 $\widehat{c}_m$ 是 $R_m$ 上所有输入实例 $x_i$ 对应的输出 $y_i$ 的均值。所以上式中的 $c_1$ 和 $c_2$ ，其最优值分别是：

$$\widehat{c}_1 = avg(y_i|x_i \in R_1(j,s))\quad 和\quad \widehat{c}_2 = avg(y_i|x_i \in R_2(j,s))$$

遍历所有输入变量，找到最优的切分变量 $j$ ，构成一个对 $(j,s)$ 。依次将空间划分为两个区域。接着，对每个区域重复上述划分过程，直到满足停止条件为止。这样就生成了一棵回归树。这样的回归树通常称为最小二乘回归树。

##### 4.1.2 分类树的生成

分类树用基尼系数选择最优特征，同时决定该特征的最优二值切分点。

**基尼指数**（Gini index）

分类问题中，假设有 $K$ 个类，样本点属于第 $k$ 类的概率为 $p_k$ ，则概率分布的基尼指数定义为

$$Gini(p)=\sum_{k=1}^K p_k(1-p_k)=1-\sum_{k=1}^K p_k^2$$

对于二分类问题，若样本点属于第 1 个类的概率是 $p$ ，则概率分布的基尼指数为

$$Gini(p) = 2p(1-p)$$

对于给定的样本集合 $D$ ，其基尼指数为

$$Gini(D)=1-\sum_{k=1}^K (\frac{|C_k|}{|D|})^2$$

这里，$C_k$ 是 $D$ 中属于第 $k$ 类的样本子集，$K$ 是类的个数。

如果样本集合 $D$ 根据特征 $A$ 是否取某一可能取值 $a$ 被分割成 $D_1$ 和 $D_2$ 两部分，即

$$D_1=\{(x,y) \in D|A(x)=a\}, \quad D_2=D-D_1$$

则在特征 $A$  的条件下，集合 $D$ 的基尼指数定义为

$$Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)$$

基尼指数 $Gini(D)$ 表示集合 $D$ 的不确定性，基尼指数 $Gini(D,A)$ 表示经 $A=a$分割后集合 $D$ 的不确定性。基尼指数值越大，样本集合的不确定性也就越大，这一点与熵相似。

**CART 生成算法**

根据训练数据集，从根结点开始，递归地对每个结点进行以下操作，构建二叉决策树：

（1）设结点的训练数据集为 $D$ ，计算现有特征对该数据集的基尼指数。此时，对每一个特征 $A$ ，对其可能取的每个值 $a$ ，根据样本点 $A=a$ 的测试为 “是” 或 “否” 将 $D$ 分割成 $D_1$ 和 $D_2$ 两部分，利用上面的公式计算 $A=a$ 时的基尼指数。

（2）在所有可能的特征 $A$ 以及他们所有可能的切分点 $a$ 中，**选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点**。依最优特征与最优切分点，从现有的结点生成两个子结点，将训练集一分为二。

（3）对两个子结点递归地调用（1），（2），直到满足停止条件。

（4）生成 **CART** 决策树。

#### 4.2 CART 剪枝

**CART** 剪枝算法从“完全生长”的决策树的底端剪去一些子树，是决策树变小（模型变简单），从而能够对未知数据有更准确的预测。

**CART** 剪枝算法由两步组成：

（1）首先从生成算法产生的决策树 $T_0$ 底端开始不断剪枝，直到 $T_0$ 的根结点，形成一个子树序列 $\{T_0,T_1,...,T_n\}$；

（2）然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树。

### 最后

介绍决策树学习方法的资料有很多，更详细的可以参考——李航《统计学习方法》--第2版。或其他学习资料。

