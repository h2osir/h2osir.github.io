[ { "title": "决策树的基本原理", "url": "/posts/decision-tree/", "categories": "机器学习算法原理, 决策树", "tags": "机器学习, 决策树", "date": "2021-10-18 21:19:00 +0800", "snippet": "决策树是一种基本的分类与回归方法。此处简单介绍分类的决策树。决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是 if-then 规则的集合。其主要优点是模型具有可读性，分类速度快。学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。预测时，对新的数据，利用决策树模型进行分类。决策树学习通常包括 3 个步骤：特征选择、决策树的生成和决策树的修剪。1. 特征选择特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率。通常特征选择的准则是信息增益 或 信息增益比 。举个例子来说明特征选择问题表1. 贷款信息表 ID 年龄 有工作 有自己的房子 信贷情况 类别 1 青年 否 否 一般 否 2 青年 否 否 好 否 3 青年 是 否 好 是 4 青年 是 是 一般 是 5 青年 否 否 一般 否 6 中年 否 否 一般 否 7 中年 否 否 好 否 8 中年 是 是 好 是 9 中年 否 是 非常好 是 10 中年 否 是 非常好 是 11 老年 否 是 非常好 是 12 老年 否 是 好 是 13 老年 是 否 好 是 14 老年 是 否 非常好 是 15 老年 否 否 一般 否 表1是一个由15个样本组成的贷款申请训练数据。希望通过所给的训练数据学习一个贷款申请的决策树，用以对未来的贷款申请进行分类，即当新的客户提出贷款申请时，根据申请人的特征利用决策树决定是否批准贷款申请。特征选择是决定用哪个特征来划分特征空间。信息增益为了便于说明，先给出熵与条件熵的定义。在信息论与概率统计中，熵（entropy）是表示随机变量不确定性的度量。设 $X$ 是一个取有限个值的离散随机变量，其概率分布为\\[P(X=x_i)=p_i, i=1,2,...,n\\]则随机变量 $X$ 的熵定义为\\[H(X)=-\\sum_{i=1}^{n}p_i·\\log_2p_i\\]上式中对数的底数可以是2或者e，这时熵的单位分别称作比特（bit）或纳特（nat）。若 $p_i=0$ ，则定义 $0log_20=0$ 。由定义可知，熵只依赖与 $X$ 的分布，而与 $X$ 的取值无关，所以也可以将 $X$ 的熵记作 $H(p)$ ，即：\\[H(p)=-\\sum_{i=1}^{n}p_i·\\log_2p_i\\]熵越大，随机变量的不确定性就越大。从定义可验证\\[0\\leq{H(p)}\\leq{\\log_2n}\\]当随机变量只取两个值，例如 1，0 时，即 $X$ 的分布为\\[P(X=1)=p,\\quad P(X=0)= 1-p,\\quad 0 \\leq p \\leq 1\\]则 $X$ 的熵为\\[H(p)=-p\\log_2p-(1-p)\\log_2(1-p)\\]设有随机变量 $(X,Y)$ ，其联合概率分布为\\[P(X=x_i,Y=y_i)=p_{ij}, \\quad i=1,2,...,n;\\quad j=1,2,...,m\\]则随机变量 $X$ 给定的条件下随机变量 $Y$ 的条件熵 $H(Y|X)$ 定义为：$X$ 给定条件下 $Y$ 的条件概率分布的熵对 $X$ 的数学期望\\[H(Y|X)=\\sum_{i=1}^{n}p_i·H(Y|X=x_i)\\]这里，$p_i=P(X=x_i),i=1,2,…,n$ 。信息增益（information gain）表示得知特征 $X$ 的信息而使得类 $Y$ 的信息的不确定性减少的程度。特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D,A)$，定义为集合 $D$ 的熵 $H(D)$ 与特征 $A$ 给定的条件下 $D$ 的条件熵 $H(D|A)$ 之差，即：\\[g(D,A)=H(D)-H(D|A)\\]显然，对于数据集 $D$ 而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益。信息增益越大的特征具有更强的分类能力。因此，根据信息增益准则的特征选择方法是：对训练数据集（或子集）$D$ ，计算其每个特征的信息增益，并比较他们的大小，选择信息增益最大的特征。前方高能，请打开脑洞！！！设： 训练数据集为 $D$，$|D|$表示其样本容量，即样本的个数。 有 $K$ 个类别 $C_k ，k=1,2,…,K$，$|C_k|$属于类 $C_k$ 的样本个数，则有：\\[\\sum_{k=1}^{K}|C_k|=|D|\\] 特征 $A$ 有 $n$ 个不同的取值 ${a_1,a_2,…,a_n}$ ，根据特征 $A$ 的取值，将 $D$ 划分为 $n$ 个子集 $D_1,D_2,…,D_n$，$|D_i|$ 为 $D_i$ 的样本个数，则有：\\[\\sum_{i=1}^n{|D_i|}=|D|\\] 记子集 $D_i$ 中属于类 $C_k$ 的样本的集合为 $D_{ik}$ ，即$D_{ik}=D_i\\bigcap{C_k}$，$|D_{ik}|$为 $D_{ik}$ 的样本个数。 于是信息增益的算法如下。信息增益的算法 计算数据集 $D$ 的熵（经验熵）$H(D)$\\[H(D)=-\\sum_{k=1}^K\\frac{|C_k|}{|D|}\\log_2 \\frac{|C_k|}{|D|}\\] 计算特征 $A$ 对数据集 $D$ 的条件熵（经验条件熵）\\(H(D\\|A)\\)\\[H(D|A)=\\sum_{i=1}^n \\frac{|D_i|}{|D|}H(D_i)=-\\sum_{i=1}^n \\frac{|D_i|}{|D|}\\sum_{k=1}^K\\frac{|D_{ik}|}{|D_i|}\\log_2 \\frac{|D_{ik}|}{|D_i|}\\] 计算信息增益\\[g(D,A)=H(D)-H(D|A)\\] 举个栗子：对表1 所给的训练数据集 $D$ ，根据信息增益准则选择最优特征。解：首先计算数据集 $D$ 的熵 $H(D)$ 。\\[H(D)=-\\frac{9}{15} \\log_2{\\frac{9}{15}}-\\frac{6}{15} \\log_2{\\frac{6}{15}}=0.971\\]然后计算各特征对数据集 $D$ 的信息增益。分别以 $A_1,A_2,A_3,A_4$ 表示年龄、有工作、有自己的房子和信贷情况 4 个特征，则：（1）分别以 $1,2,3$ 表示 $A_1$ 的三个取值，则：\\[p_1 = \\frac{5}{15},\\quad p_2 = \\frac{5}{15},\\quad p_3 = \\frac{5}{15}\\]然后计算特征 $A_1$ 对数据集 $D$ 的条件熵 $H(D|A_1)$ ，即：\\[\\begin{equation} \\begin{aligned} H(D|A_1)&amp;amp;=p_1H(D_1)+p_2H(D_2)+p_3H(D_3)\\\\&amp;amp;=[\\frac{5}{15}(-\\frac{2}{5}\\log_2\\frac{2}{5}-\\frac{3}{5}\\log_2\\frac{3}{5}) + \\frac{5}{15}(-\\frac{3}{5}\\log_2\\frac{3}{5}-\\frac{2}{5}\\log_2\\frac{2}{5}) + \\frac{5}{15}(-\\frac{4}{5}\\log_2\\frac{4}{5}-\\frac{1}{5}\\log_2\\frac{1}{5})]\\\\ &amp;amp;=0.888 \\end{aligned} \\end{equation}\\]接下来计算特征 $A_1$ 对数据集 $D$ 的信息增益，即：\\[\\begin{equation} \\begin{aligned} g(D,A_1)&amp;amp;=H(D)=H(D|A_1)\\\\&amp;amp;=0.971-0.888\\\\&amp;amp;=0.083 \\end{aligned} \\end{equation}\\]（2）\\[\\begin{equation} \\begin{aligned} g(D,A_2)&amp;amp;=H(D)-[\\frac{5}{15}H(D_1)+\\frac{10}{15}H(D_2)]\\\\&amp;amp;=0.971-[\\frac{5}{15}\\times0+\\frac{10}{15}(-\\frac{4}{10}\\log_2\\frac{4}{10}-\\frac{6}{10}\\log_2\\frac{6}{10})]\\\\&amp;amp;=0.324 \\end{aligned} \\end{equation}\\]同理，可以算出（3）和（4），分别是：\\[g(D,A_3)=0.420,\\quad g(D,A_4)=0.363\\]最后，比较各特征的信息增益值。由于特征 $A_3$ 的信息增益值最大，所以选择特征 $A_3$ 作为最优特征。信息增益比以信息增益作为划分训练集的特征的依据，存在偏向于选择取值较多的特征的问题，因为取值越多，信息增益就会越大。而使用信息增益比（information gain ratio）可以对这一问题进行校正。这是特征选择的另一准则。特征 $A$ 对训练数据集 $D$ 的信息增益比 $g_R(D,A)$，定义为其信息增益 $g(D,A)$ 与训练集 $D$ 关于特征 $A$ 的值的熵 $H_A(D)$ 之比，即\\[g_R(D,A)=\\frac{g(D,A)}{H_A(D)}\\]其中，\\[H_A(D)=-\\sum_{i=1}^n \\frac{|D_i|}{|D|} \\log_2 \\frac{|D_i|}{|D|}\\]$n$ 是特征 $A$ 取值的个数。2. 决策树的生成2.1 ID3 算法ID3 算法的核心是在决策树各个结点上应用信息增益正则选择特征，递归地构建决策树。具体方法是：从根节点（root node）开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点；再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小（小于某个阈值）或没有特征可以选择为止。最后得到一颗决策树。ID3 相当于用极大似然法进行概率模型的选择。ID3 算法只有树的生成，所以该算法生成的树容易产生过拟合。2.2 C4.5 算法C4.5 算法与 ID3 算法相似，C4.5 算法对 ID3 算法进行了改进。C4.5 在生成的过程中，采用信心增益比准则来选择特征。3. 决策树的剪枝决策树生成算法递归地产生决策树，直到不能继续下去为止。这样产生的决策树往往对训练集数据的分类很准确，但对未知的测试数据的分类却没有那么准确，即出现过拟合现象。解决过拟合的办法是考虑决策树的复杂度，对已生成的决策树进行简化。在决策树学习中将已生成的树进行简化的过程称为剪枝（pruning）。这里介绍一种简单的决策树的剪枝算法。决策树的剪枝往往通过极小化决策树整体的损失函数来实现。设：树 $T$ 的叶结点个数为 $|T|$ ，$t$ 是树 $T$ 的叶结点，该叶结点有 $N_t$ 个样本点，其中 $k$ 类的样本点有 $N_{tk}$ 个，$k=1,2,…,K$，\\(Ht(T)\\) 为叶结点 $t$ 上的熵，$\\alpha \\geq 0$ 为参数，则决策树学习的损失函数可以定义为\\[C_\\alpha (T)=\\sum_{t=1}^{|T|} N_tH_t(T)+\\alpha|T|\\]其中，\\[H_t(T)=-\\sum_{k=1}^K \\frac{N_{tk}}{N_t} \\log_2 \\frac{N_{tk}}{N_t}\\]在损失函数中，将等式右端的第 1 项记作 $C(T)$ ，这时有：\\[C_\\alpha (T)=C(T)+\\alpha|T|\\]$C(T)$ 表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，$|T|$ 表示模型复杂度，参数 $\\alpha \\geq 0$ 控制两者之间的影响 。可以看出，决策树生成只考虑了通过提高信息增益（或信息增益比）对训练数据进行更好的拟合。而决策树剪枝通过优化损失函数还考虑了减小模型复杂度。决策树生成学习局部的模型，而决策树剪枝学习整体的模型。损失函数的极小化等价与正则化的极大似然估计。所以，利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。决策树的剪枝算法： （1）计算每个结点的熵 （2）递归地从树的叶结点向上回缩 设一组叶结点回缩到其父结点之前与之后的整体树分别是 $T_B$ 与 $T_A$ ，其对应的损失函数值分别是 $C_\\alpha(T_B)$ 与 $C_\\alpha(T_A)$ ，如果 $C_\\alpha(T_B) \\geq C_\\alpha(T_A)$ ，则进行剪枝，即将父结点变为新的叶结点。 （3）返回（2），直到不能继续为止，得到损失函数最小的子树 $T_\\alpha$ 。 4. CART 算法分类与回归（classification and regression tree， CART）模型由 Breiman 等人在1984年提出，是应用广泛的决策树学习方法。CART 算法假设决策树是二叉树，内部结点的特征的取值为 “是” 和 “否”，左分支是取值为 “是” 的分支，右分支是取值为 “否” 的分支。前面介绍的 ID3 和 C4.5 算法，都只是生成决策树。而 CART 算法由两步组成：（1）决策树生成：基于训练集生成决策树，生成的决策树要尽量大；（2）决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。4.1 CART 生成决策树的生成就是递归地构建二叉决策树的过程。对回归树用平方误差最小化准则，对分类树用基尼指数（Gini index）最小化准则，进行特征选择，生成二叉树。此处只介绍分类树的生成4.1.1 回归树的生成假设 $X$ 与 $Y$ 分别为输入和输出变量，并且 $Y$ 是连续变量。考虑如何生成回归树。选择第 $j$ 个变量 $x^j$ 和它的一个取值 $s$ ，作为切分变量和切分点，并定义两个区域：\\[R_1(j,s) = \\{x|x^{(j)}\\leq s\\},\\quad R_2(j,s) = \\{x|x^{(j)} &amp;gt; s\\}\\]然后寻找最优变量 $j$ 和最优的切分点 $s$ 。具体地，求解\\[\\min_{j,s} [\\min_{c_1} \\sum_{x_i \\in R_{1(j,s)}}(y_i-c_1)^2+\\min_{c_2} \\sum_{x_i \\in R_{2(j,s)}}(y_i-c_2)^2]\\]用平方误差最小的准则求解每个单元上的最优输出值。易知，单元 $R_m$ 上的 $c_m$ 的最优值 $\\widehat{c}_m$ 是 $R_m$ 上所有输入实例 $x_i$ 对应的输出 $y_i$ 的均值。所以上式中的 $c_1$ 和 $c_2$ ，其最优值分别是：\\[\\widehat{c}_1 = avg(y_i|x_i \\in R_1(j,s))\\quad 和\\quad \\widehat{c}_2 = avg(y_i|x_i \\in R_2(j,s))\\]遍历所有输入变量，找到最优的切分变量 $j$ ，构成一个对 $(j,s)$ 。依次将空间划分为两个区域。接着，对每个区域重复上述划分过程，直到满足停止条件为止。这样就生成了一棵回归树。这样的回归树通常称为最小二乘回归树。4.1.2 分类树的生成分类树用基尼系数选择最优特征，同时决定该特征的最优二值切分点。基尼指数（Gini index）分类问题中，假设有 $K$ 个类，样本点属于第 $k$ 类的概率为 $p_k$ ，则概率分布的基尼指数定义为\\[Gini(p)=\\sum_{k=1}^K p_k(1-p_k)=1-\\sum_{k=1}^K p_k^2\\]对于二分类问题，若样本点属于第 1 个类的概率是 $p$ ，则概率分布的基尼指数为\\[Gini(p) = 2p(1-p)\\]对于给定的样本集合 $D$ ，其基尼指数为\\[Gini(D)=1-\\sum_{k=1}^K (\\frac{|C_k|}{|D|})^2\\]这里，$C_k$ 是 $D$ 中属于第 $k$ 类的样本子集，$K$ 是类的个数。如果样本集合 $D$ 根据特征 $A$ 是否取某一可能取值 $a$ 被分割成 $D_1$ 和 $D_2$ 两部分，即\\[D_1=\\{(x,y) \\in D|A(x)=a\\}, \\quad D_2=D-D_1\\]则在特征 $A$ 的条件下，集合 $D$ 的基尼指数定义为\\[Gini(D,A)=\\frac{|D_1|}{|D|}Gini(D_1)+\\frac{|D_2|}{|D|}Gini(D_2)\\]基尼指数 $Gini(D)$ 表示集合 $D$ 的不确定性，基尼指数 $Gini(D,A)$ 表示经 $A=a$分割后集合 $D$ 的不确定性。基尼指数值越大，样本集合的不确定性也就越大，这一点与熵相似。CART 生成算法根据训练数据集，从根结点开始，递归地对每个结点进行以下操作，构建二叉决策树：（1）设结点的训练数据集为 $D$ ，计算现有特征对该数据集的基尼指数。此时，对每一个特征 $A$ ，对其可能取的每个值 $a$ ，根据样本点 $A=a$ 的测试为 “是” 或 “否” 将 $D$ 分割成 $D_1$ 和 $D_2$ 两部分，利用上面的公式计算 $A=a$ 时的基尼指数。（2）在所有可能的特征 $A$ 以及他们所有可能的切分点 $a$ 中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现有的结点生成两个子结点，将训练集一分为二。（3）对两个子结点递归地调用（1），（2），直到满足停止条件。（4）生成 CART 决策树。4.2 CART 剪枝CART 剪枝算法从“完全生长”的决策树的底端剪去一些子树，是决策树变小（模型变简单），从而能够对未知数据有更准确的预测。CART 剪枝算法由两步组成：（1）首先从生成算法产生的决策树 $T_0$ 底端开始不断剪枝，直到 $T_0$ 的根结点，形成一个子树序列 ${T_0,T_1,…,T_n}$；（2）然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树。最后介绍决策树学习方法的资料有很多，更详细的可以参考——李航《统计学习方法》–第2版。或其他学习资料。" }, { "title": "逻辑回归的基本原理", "url": "/posts/logistic-regression/", "categories": "机器学习算法原理, 逻辑回归", "tags": "逻辑回归, 机器学习", "date": "2021-10-07 05:45:35 +0800", "snippet": "逻辑回归是统计学习中的经典分类方法。属于对数线型模型。分为二项逻辑回归和多项逻辑回归，这里主要介绍二项逻辑回归模型。二项逻辑回归模型二项逻辑回归模型，即二分类模型。由条件概率分布 $P(Y|X)$ 表示，随机变量 $X$ 的取值为实数，随机变量 $Y$ 的取值为 1 或 0。二项逻辑回归模型是如下的条件概率分布：\\[P(Y=1|x)=\\frac{e^{w·x+b}}{1+e^{w·x+b}}\\quad\\quad\\quad\\quad\\quad\\quad\\quad(1)\\]\\[P(Y=0|x)=\\frac{1}{1+e^{w·x+b}}\\quad\\quad\\quad\\quad\\quad\\quad\\quad(2)\\]这里，$x \\in R^n$ 是输入，$Y \\in {0,1}$ 是输出，$w \\in R^n$ 和 $b \\in R$ 是参数，$w$ 称为权值向量，$b$ 称为偏置，$w·x$ 为 $w$ 和 $x$ 的内积。逻辑回归比较两个概率值 $P(Y=1|x)$ 与 $P(Y=0|x)$ 的大小，将实例 $x$ 分到概率值较大的那一类。有时为了方便，将权值向量和输入向量加以扩充，仍记作 $w,x$ ，即 $w=(w^{(1)},w^{(2)},…,w^{(n)},b)^T,x=(x^{(1)},x^{(2)},…,x^{(n)},1)^T$ 。此时：\\[P(Y=1|x)=\\frac{e^{w·x}}{1+e^{w·x}}\\]\\[P(Y=0|x)=\\frac{1}{1+e^{w·x}}\\]现在考察逻辑回归模型的特点。一个事件的几率（odds）是指该事件发生的概率与该事件不发生的概率的比值。如果事件发生的概率为 $p$ ，那么该事件的几率是 $\\frac{p}{1-p}$ ，该事件的对数几率（log odds）或 logit 函数（很多资料叫 sigmoid 函数）是\\[logit(p)=\\log\\frac{p}{1-p}\\]对逻辑回归而言，\\[\\log \\frac{P(Y=1|x)}{1-P(Y=1|x)}=w·x\\]这就是说，在逻辑回归模型中，输出 $Y=1$ 的对数几率是输入 $x$ 的线性函数。或者说，输出 $Y=1$ 的对数几率是由输入 $x$ 的线性函数表示的模型，即逻辑回归模型。模型参数估计极大化似然函数估计极大化似然函数估计，又叫 ”极大似然估计“ 。是根据求解关于 $w，这里 w=(w^{(1)},w^{(2)},…,w^{(n)}, b)^T$ 的函数的极大值来得到 $w$ 的估计值 $\\widehat w$ 。逻辑回归模型学习时，应用极大似然估计法估计模型参数，从而得到逻辑回归模型。设：\\[P(Y=1|x)=\\pi (x), \\quad P(Y=0|x)=1- \\pi (x)\\]由于 $Y$ 只能取 0 或者 1 ，则似然函数可以巧妙地写成下面这种形式\\[\\prod_{i=1}^N [\\pi (x_i)]^{y_i}[1-\\pi(x_i)]^{1-y_i}\\]其中，$N$ 是样本容量，$y_i$ 是第 $i$ 个样本的真实类别（0 或 1），$\\pi (x_i)$ 是模型对第 $i$ 个样本的预测为类别 1 的概率（下同）。对上式取对数，则对数似然函数为\\[\\begin{equation} \\begin{aligned} L(w)&amp;amp;=\\sum_{i=1}^N [y_i \\log \\pi(x_i) + (1-y_i)\\log(1-\\pi(x_i))] \\\\\\\\&amp;amp;=\\sum_{i=1}^N [y_i \\log \\frac{\\pi(x_i)}{1-\\pi(x_i)} + \\log(1-\\pi(x_i))] \\\\\\\\&amp;amp;=\\sum_{i=1}^N [y_i(w·x_i)-\\log(1+e^{w·x_i})]\\end{aligned} \\end{equation}\\]上式中，$x_i, y_i$ 均已知，$w$ 为未知变量，对 $L(w)$ 求极大值，得到 $w$ 的估计值 $\\widehat w$ ，代入上面的（1），（2）式即为学习到的逻辑回归模型。极小化损失函数估计在常用的损失函数中，逻辑回归采用的损失函数为对数损失函数或对数似然损失函数，其公式为：\\[L(Y, P(Y|X))= -\\log{P(Y|X)}\\]而对于二分类逻辑回归模型，得到的 $y$ 要么是1，要么是0。假设对于样本 $x$ ，估计 $y=1$ 的概率是 $\\widehat p$ ，则可以将损失函数分成两类： 如果 $x$ 的真实类别 $y=1$，则：估计出来的概率 $\\widehat p$ 越小，损失函数就越大（估计错误） 如果 $x$ 的真实类别 $y=0$，则：估计出来的概率 $\\widehat p$ 越大，损失函数就越大（估计错误）那么根据对数损失函数的定义公式，损失函数可写成如下形式：\\[J=\\begin{cases}-\\log \\widehat p\\quad\\quad\\quad\\quad\\quad if\\quad Y=1\\\\\\\\ -\\log(1- \\widehat p)\\quad\\quad\\ \\,\\ if \\quad Y=0\\end{cases}\\]由于模型是个二分类问题，分类结果 $y$ 非 0 即 1 ，因此我们可以使用一个巧妙的方法，通过控制系数的方式，将上面的两个式子合并成一个：\\[J(\\widehat p,y)=-\\log(\\widehat p)^y-\\log(1-\\widehat p)^{1-y}\\]化简上式，得\\[J(\\widehat p,y)=-y\\log(\\widehat p)-(1-y)\\log(1-\\widehat p)\\]以上是对于单个样本的损失函数，那么扩展到整个数据集，其损失函数则是：\\[\\begin{equation} \\begin{aligned}J(\\widehat p,y)&amp;amp;=-\\frac{1}{N}\\sum_{i=1}^N [y_{i}\\log(\\widehat p^{(i)})+(1-y_{i})\\log(1-\\widehat p^{(i)})]\\\\\\\\&amp;amp;=-\\frac{1}{N}\\sum_{i=1}^N [y_i \\log \\frac{\\widehat p^{(i)}}{1-\\widehat p^{(i)}} + \\log(1-\\widehat p^{(i)})]\\end{aligned} \\end{equation}\\]这里，$N$ 是样本容量，$y_i$ 是第 $i$ 个样本的真实类别，$\\widehat p^{(i)}$ 是模型对第 $i$ 个样本的预测为类别 1 的概率。将下面两式\\[\\log \\frac{\\widehat p^{(i)}}{1-\\widehat p^{(i)}} = w·x_i,\\quad (1-\\widehat p^{(i)})=\\frac{1}{1+e^{w·x_i}}\\]代入 $J(\\widehat p, y)$ ，得：\\[J(w)=-\\frac{1}{N} \\sum_{i=1}^N [y_i(w·x_i)-\\log(1+e^{w·x_i})]\\]这就是逻辑回归的损失函数。通过求解 $J(w)$ 的最小值，来估计参数 $w$ 的估计值。最优化求解算法现在，我们知道了逻辑回归模型中估计参数的两种方法，极大化似然函数估计和极小化损失函数估计。那么如何求解这个最值？这是一个多元函数的无约束最值求解问题，$J(w)$ 中的 $w=(w^{(1)},w^{(2)},…,w^{(n)}, b)^T$ ，含有 $n+1$ 个元素，我们需要通过求解函数的最值来估计出 $(w^{(1)},w^{(2)},…,w^{(n)}, b)^T$ 中各个元素的估计值，作为最终的模型参数。这里主要介绍一种常用的方法——梯度下降法。梯度下降法是一种求解函数无约束最小值问题的方法；而梯度上升法则是求解函数无约束最大值问题的方法。二者都属于迭代算法，其迭代公式分别为：梯度下降法：\\[w:=w-\\alpha \\frac{\\partial J(w)}{\\partial w}\\]梯度上升法：\\[w:=w+\\alpha \\frac{\\partial J(w)}{\\partial w}\\]这里引入两个概念： 梯度（或梯度方向） 上式中的 $\\frac{\\partial J(w)}{\\partial w}$ 就是梯度，它的结果也是一个向量，含有 $n+1$ 个元素，它表示下一步迭代的方向。即下降（或上升）的方向。有了方向，我们还需要知道在这个方向上”走多远”，这就是下面提到的步长。 步长（或学习率） 上式中的 $\\alpha$ 称为步长（学习率），该值太大局部线性近似就不成立，偏差较大；该值太小收敛太慢，导致一个长时间的训练。所以后续调参需要注意该值的选择。 对于给到的一个初始值 $w_0$ ，$J(w)$ 对 $w_0$ 中的每一个元素求一阶偏导，便会得到 $J(w)$ 在 $w_0$ 中每一个元素处的一阶偏导值，再由迭代公式进行迭代，直到 $J(w)$ 收敛。总结关于最优化求解算法，除了梯度下降和梯度上升之外，还有其他的算法，比如牛顿法、拟牛顿法等。深度地理解某一种算法，对于理解其他算法也会有一定的帮助。" } ]
